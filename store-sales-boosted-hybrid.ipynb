{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport datetime\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_log_error\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-10T05:57:00.252553Z","iopub.execute_input":"2022-07-10T05:57:00.253646Z","iopub.status.idle":"2022-07-10T05:57:00.927173Z","shell.execute_reply.started":"2022-07-10T05:57:00.253568Z","shell.execute_reply":"2022-07-10T05:57:00.925885Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Plot Style","metadata":{}},{"cell_type":"code","source":"plt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True, figsize=(11, 4))\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\nplot_params = dict(\n    color=\"0.75\",\n    style=\".-\",\n    markeredgecolor=\"0.25\",\n    markerfacecolor=\"0.25\",\n    legend=False,\n)\n%config InlineBackend.figure_format = 'retina'","metadata":{"execution":{"iopub.status.busy":"2022-07-10T05:57:00.934266Z","iopub.execute_input":"2022-07-10T05:57:00.934738Z","iopub.status.idle":"2022-07-10T05:57:00.954826Z","shell.execute_reply.started":"2022-07-10T05:57:00.934691Z","shell.execute_reply":"2022-07-10T05:57:00.953296Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Load Files","metadata":{}},{"cell_type":"code","source":"IN_FP = '../input/store-sales-time-series-forecasting'\nTRAIN_FP = os.path.join(IN_FP, 'train.csv')\nTEST_FP = os.path.join(IN_FP, 'test.csv')\nHOLIDAYS_FP = os.path.join(IN_FP, 'holidays_events.csv')","metadata":{"execution":{"iopub.status.busy":"2022-07-10T05:57:00.957230Z","iopub.execute_input":"2022-07-10T05:57:00.957787Z","iopub.status.idle":"2022-07-10T05:57:00.964208Z","shell.execute_reply.started":"2022-07-10T05:57:00.957737Z","shell.execute_reply":"2022-07-10T05:57:00.963234Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"store_sales = pd.read_csv(\n    TRAIN_FP,\n    usecols=['store_nbr', 'family', 'date', 'sales', 'onpromotion'],\n    dtype={\n        'store_nbr': 'category',\n        'family': 'category',\n        'sales': 'float32',\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True,\n)\n\nstore_sales['date'] = store_sales.date.dt.to_period('D')\nstore_sales = store_sales.set_index(['store_nbr', 'family', 'date']).sort_index()\naverage_sales = store_sales.groupby('date').mean()['sales']\n\nfamily_sales = (\n    store_sales\n    .groupby(['family', 'date'])\n    .mean()\n    .unstack('family')\n    .loc['2017']    \n)\n\ntest = pd.read_csv(\n    TEST_FP,\n    dtype={\n        'store_nbr': 'category',\n        'family': 'category',\n        'onpromotion': 'uint32',\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True,\n)\n\ntest['date'] = test.date.dt.to_period('D')\ntest = test.set_index(['store_nbr', 'family', 'date']).sort_index()\n\nholidays_events = pd.read_csv(\n    HOLIDAYS_FP,\n    dtype={\n        'type': 'category',\n        'locale': 'category',\n        'locale_name': 'category',\n        'description': 'category',\n        'transferred': 'bool',\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True,\n)\nholidays_events['date'] = holidays_events['date'].replace({'2013-04-29':pd.to_datetime('2013-03-29')})\nholidays_events = holidays_events.set_index('date').to_period('D')","metadata":{"execution":{"iopub.status.busy":"2022-07-10T05:57:00.966080Z","iopub.execute_input":"2022-07-10T05:57:00.966546Z","iopub.status.idle":"2022-07-10T05:57:05.447946Z","shell.execute_reply.started":"2022-07-10T05:57:00.966500Z","shell.execute_reply":"2022-07-10T05:57:05.446716Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Create Calender","metadata":{}},{"cell_type":"code","source":"# Creating workday calender using holidays\n# https://www.kaggle.com/dkomyagin/simple-ts-ridge-rf\n\ncalender = pd.DataFrame(index=pd.date_range('2013-01-01', '2017-08-31')).to_period('D')\ncalender['dofw'] = calender.index.dayofweek\n\ndf_hev = holidays_events[holidays_events.locale == 'National']  # Only considering national holidays in this instance\ndf_hev = df_hev.groupby(df_hev.index).first()\n\ncalender['wd'] = True\ncalender.loc[calender.dofw > 4, 'wd'] = False  # Omit weekends\n\ncalender = calender.merge(df_hev, how='left', left_index=True, right_index=True)\n\ncalender.loc[calender.type == 'Bridge', 'wd'] = False\ncalender.loc[calender.type == 'Work Day', 'wd'] = True\ncalender.loc[calender.type == 'Transfer', 'wd'] = False\ncalender.loc[(calender.type == 'Holiday') & (calender.transferred == False), 'wd'] = False\ncalender.loc[(calender.type == 'Holiday') & (calender.transferred == True), 'wd'] = True","metadata":{"execution":{"iopub.status.busy":"2022-07-10T05:57:05.451818Z","iopub.execute_input":"2022-07-10T05:57:05.452352Z","iopub.status.idle":"2022-07-10T05:57:05.514968Z","shell.execute_reply.started":"2022-07-10T05:57:05.452301Z","shell.execute_reply":"2022-07-10T05:57:05.513630Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Problem definition","metadata":{}},{"cell_type":"code","source":"print(\"Training Data\\n\", store_sales, \"\\n\\n\")\nprint(\"Test Data\\n\", test)","metadata":{"execution":{"iopub.status.busy":"2022-07-10T05:57:05.516729Z","iopub.execute_input":"2022-07-10T05:57:05.517611Z","iopub.status.idle":"2022-07-10T05:57:05.537898Z","shell.execute_reply.started":"2022-07-10T05:57:05.517570Z","shell.execute_reply":"2022-07-10T05:57:05.536847Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Training Data\n                                      sales  onpromotion\nstore_nbr family     date                              \n1         AUTOMOTIVE 2013-01-01   0.000000            0\n                     2013-01-02   2.000000            0\n                     2013-01-03   3.000000            0\n                     2013-01-04   3.000000            0\n                     2013-01-05   5.000000            0\n...                                    ...          ...\n9         SEAFOOD    2017-08-11  23.830999            0\n                     2017-08-12  16.859001            4\n                     2017-08-13  20.000000            0\n                     2017-08-14  17.000000            0\n                     2017-08-15  16.000000            0\n\n[3000888 rows x 2 columns] \n\n\nTest Data\n                                       id  onpromotion\nstore_nbr family     date                            \n1         AUTOMOTIVE 2017-08-16  3000888            0\n                     2017-08-17  3002670            0\n                     2017-08-18  3004452            0\n                     2017-08-19  3006234            0\n                     2017-08-20  3008016            0\n...                                  ...          ...\n9         SEAFOOD    2017-08-27  3022271            0\n                     2017-08-28  3024053            0\n                     2017-08-29  3025835            0\n                     2017-08-30  3027617            0\n                     2017-08-31  3029399            0\n\n[28512 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Forecast Requirements and Considerations\n- 16 day forecast with 1 step lead time from 16th August 2017 to 31st August 2017\n- Large parts of information missing prior to June 2015 so removed from training set\n- Validation using the last 16 days of the training set","metadata":{}},{"cell_type":"code","source":"train_start_day = datetime.datetime(2015, 6, 16)\ntrain_end_day = datetime.datetime(2017, 7, 30)\nfull_train_end_day = datetime.datetime(2017, 8, 15)\n\nval_start_day = datetime.datetime(2017, 7, 31)\nval_end_day = full_train_end_day\n\ntest_start_day = datetime.datetime(2017, 8, 16)\ntest_end_day = datetime.datetime(2017, 8, 31)","metadata":{"execution":{"iopub.status.busy":"2022-07-10T05:57:05.539067Z","iopub.execute_input":"2022-07-10T05:57:05.539667Z","iopub.status.idle":"2022-07-10T05:57:05.547040Z","shell.execute_reply.started":"2022-07-10T05:57:05.539624Z","shell.execute_reply":"2022-07-10T05:57:05.545625Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Boosted Hybrid Class","metadata":{}},{"cell_type":"code","source":"class BoostedHybrid:\n    def __init__(self, model1, model2):\n        self.model1 = model1\n        self.model2 = model2\n        self.y_columns = None\n        self.stack_cols = None\n        self.y_resid = None\n    \n    def fit1(self, X1, y, stack_cols=None):\n        self.model1.fit(X1, y)\n        \n        y_fit = pd.DataFrame(\n            self.model1.predict(X1),\n            index=X1.index,\n            columns=y.columns        \n        )\n        \n        self.y_resid = y - y_fit\n        self.y_resid = self.y_resid.stack(stack_cols).squeeze()\n        \n    def fit2(self, X2, ignore_first_n_rows, stack_cols=None):\n        self.model_2.fit(X2.iloc[ignore_first_n_rows*1782:, :], self.y_resid.iloc[ignore_first_n_rows*1782:])\n        self.y_columns = y.columns\n        self.stack_cols = stack_cols\n    \n    def predict(self, X1, X2, ignore_first_n_rows):\n        y_pred = pd.DataFrame(\n            self.model1.predict(X1.iloc[ignore_first_n_rows:, :]),\n            index=X1.iloc[ignore_first_n_rows:, :].index,\n            columns=self.y_columns\n        )\n        \n        y_pred = y_pred.stack(self.stack_cols).squeeze()\n        y_pred += self.model2.predict(X2.iloc[ignore_first_n_rows*1782:, :])\n        \n        return y_pred.unstack(self.stack_cols)\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-07-10T05:57:05.561122Z","iopub.execute_input":"2022-07-10T05:57:05.562155Z","iopub.status.idle":"2022-07-10T05:57:05.579997Z","shell.execute_reply.started":"2022-07-10T05:57:05.562117Z","shell.execute_reply":"2022-07-10T05:57:05.578361Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## X1 Preparation\nCreation of features for trend and seasonality","metadata":{}},{"cell_type":"code","source":"def create_X1_dp_features(df):\n    y = df.loc[:, 'sales']\n    fourier = CalendarFourier(freq='M', order=4)\n    \n    dp = DeterministicProcess(\n        index=y.index,\n        constant=True,\n        order=1,\n        seasonal=True,\n        additional_terms=[fourier],\n        drop=True    \n    )\n    \n    return y, dp\n\ndef create_X1_day_features(X1, start_date, end_date, is_test_set=True):\n    if is_test_set:\n        X1 = X1.rename_axis('date')\n    X1['NewYear'] = (X1.index.dayofyear == 1)\n    X1['Christmas'] = (X1.index=='2016-12-25') | (X1.index=='2015-12-25') | (X1.index=='2014-12-25') | (X1.index=='2013-12-25')\n    X1['wd'] = calender.loc[start_date:end_date]['wd'].values\n    X1['type'] = calender.loc[start_date:end_date]['type'].values\n    X1 = pd.get_dummies(X1, columns=['type'], drop_first=False)\n    \n    return X1\n\ndef create_X1(df, start_date, end_date):\n    y, dp = create_X1_dp_features(df)\n    X1 = create_X1_day_features(dp.in_sample(), start_date, end_date, is_test_set=False)\n    \n    return X1, y, dp","metadata":{"execution":{"iopub.status.busy":"2022-07-10T05:57:18.896523Z","iopub.execute_input":"2022-07-10T05:57:18.897004Z","iopub.status.idle":"2022-07-10T05:57:18.906754Z","shell.execute_reply.started":"2022-07-10T05:57:18.896969Z","shell.execute_reply":"2022-07-10T05:57:18.905440Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## X2 Preparation\nCreation of features for categorical relationship identification","metadata":{}},{"cell_type":"code","source":"def encode_categoricals(df, columns):\n    le = LabelEncoder()\n    for col in columns:\n        df[col] = le.fit_transform(df[col])\n    return df\n\ndef create_X2_lags(ts, lags, lead_time=1, name='y', stack_cols=None):\n    ts = ts.unstack(stack_cols)\n    df = pd.concat(\n        {\n            f'{name}_lag_{i}': ts.shift(i, freq=\"D\")\n            for i in range(lead_time, lags + lead_time)\n        },\n    axis=1)\n    df = df.stack(stack_cols).reset_index()\n    df = encode_categoricals(df, stack_cols)\n    df = df.set_index('date').sort_values(by=stack_cols)\n    \n    return df\n\ndef create_X2_promo_features(df, X2):\n    df['promo_mean_rolling_7'] = df['promo_lag_1'].rolling(window=7, center=False).mean()\n    df['promo_mean_rolling_91'] = df['promo_lag_1'].rolling(window=91, center=False).median().fillna(method='bfill')\n    return X2.merge(df, on=['date', 'store_nbr', 'family'], how='left')\n\ndef create_X2_lag_features(df, X2):\n    df['y_mean_rolling_7'] = shifted_y_df['y_res_lag_1'].rolling(window=7, center=False).mean()\n    df['y_median_rolling_91'] = shifted_y_df['y_res_lag_1'].rolling(window=91, center=False).median().fillna(method='bfill')\n    return X2.merge(df, on=['date', 'store_nbr', 'family'], how='left')\n\ndef create_X2_other_features(df):\n    df['wage_day'] = (df.index.day == df.index.daysinmonth) | (df.index.day == 15)\n    df['wage_day_lag_1'] = (df.index.day == 1) | (df.index.day == 16)\n    return df\n\ndef create_X2(df, y_resid):\n    stack_columns = ['store_nbr', 'family']\n    shifted_promo_df = make_X2_lags(df.squeeze(), lags=2, name='promo', stack_cols=['store_nbr', 'family'])    \n    shifted_y_df = make_X2_lags(y_resid, lags=2, name='y_res', stack_cols=stack_solumns)\n    \n    df = df.reset_index(stack_columns)\n    X2 = encode_categoricals(df, stack_colums)\n    X2 = create_X2_other_features(X2)\n    X2 = create_X2_promo_features(shifted_promo_df, X2)\n    X2 = create_X2_lag_features(shifted_y_df, X2)\n    \n    return X2\n    ","metadata":{"execution":{"iopub.status.busy":"2022-07-10T05:58:38.259755Z","iopub.execute_input":"2022-07-10T05:58:38.260198Z","iopub.status.idle":"2022-07-10T05:58:38.275981Z","shell.execute_reply.started":"2022-07-10T05:58:38.260163Z","shell.execute_reply":"2022-07-10T05:58:38.274615Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## DirRec Forecasting Strategy","metadata":{}},{"cell_type":"markdown","source":"Based on initial testing the combination of LinearRegressor() and KNeighborsRegressor() gave the best performance on balance, so used as a starting point. [Found here](https://www.kaggle.com/jameskeogh/store-sales-time-series-forecasting)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T05:26:48.333184Z","iopub.execute_input":"2022-07-11T05:26:48.333770Z","iopub.status.idle":"2022-07-11T05:26:48.370636Z","shell.execute_reply.started":"2022-07-11T05:26:48.333656Z","shell.execute_reply":"2022-07-11T05:26:48.369053Z"}}},{"cell_type":"code","source":"model1 = LinearRegression()\nmodel2 = KNeighborsRegressor()\nmax_lag = 7","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_days = (train_end_day - train_start_day).days + 1\nvalidation_days = (val_end_day - val_start_day).days + 1\nprint(f'Training set of {training_days} days')\nprint(f'Validation set of {validation_days} days')\n\nstore_sales_in_date_range = store_sales.unstack(['store_nbr', 'family']).loc[train_start_day:train_end_day]\nstore_data_in_val_range = store_sales.unstack(['store_nbr', 'family']).loc[val_start_day:val_end_day]\ny_val = y[val_start_day:val_end_day]\n\nval_model = BoostedHybrid(model1=model1, model2=model2)","metadata":{"execution":{"iopub.status.busy":"2022-07-10T06:11:08.147510Z","iopub.execute_input":"2022-07-10T06:11:08.148060Z","iopub.status.idle":"2022-07-10T06:11:09.678096Z","shell.execute_reply.started":"2022-07-10T06:11:08.148015Z","shell.execute_reply":"2022-07-10T06:11:09.676873Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"X1_train, y_train, dp_val = create_X1(store_sales_date_range, train_start_day, train_end_day)\nval_model.fit1(X1_train, y_train, stack_cols=['store_nbr', 'family'])\n\nX2_train = create_X2(store_sales_date_range.drop('sales', axis=1).stack(['store_nbr', 'family']),\n                    val_model.y_resid)\nval_model.fit2(X2_train, max_lag, stack_cols=['store_nbr', 'family'])\n\ny_fit = val_model.predict(X1_train, X2_train, max_lag).clip(0.0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dp_X1_val_date_range = dp_val.out_of_sample(steps=validation_days)\n\nfor step in range(validation_days):\n    dp_steps_so_far = dp_X1_val_date_range.loc[val_start_day:val_start_day+pd.Timedelta(days=step),:]\n    \n    X1_combined_dp = pd.concat([dp_val.in_sample(), dp_steps_so_far])\n    X2_combined = pd.concat([store_sales_in_date_range, store_data_in_val_range.loc[val_start_day:val_start_day+pd.Timedelta(days=step), :]])\n    \n    X1_val = create_X1_day_features(X1_combined_dp, train_start_day, val_start_day+pd.Timedelta(days=step))\n    X2_val = create_X2(X2_combined.drop('sales', axis=1).stack(['store_nbr', 'family']),\n                      val_model.y_resid)\n    \n    y_pred_combined = val_model.predict(X1_val, X2_val, max_lag).clip(0.0)\n    y_plus_y_val = pd.concat([y_train, y_pred_combined.iloc[-(step+1):]])\n    \n    val_model.fit1(X1_val, y_plus_y_val, stack_cols=['store_nbr', 'family'])\n    val_model.fit2(X2_val, max_lag, stack_cols=['store_nbr', 'family'])\n    \n    rmsle_valid = mean_squared_log_error(y_val.iloc[step:step+1], y_pred_combined.iloc[-1:]) ** 0.5","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}